{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FPKoTlEyEDEx"
      },
      "outputs": [],
      "source": [
        "PATH_TO_STYLES = \"./styles\"\n",
        "PATH_TO_VGG_MODEL = \"./models\"\n",
        "PATH_TO_TRAINED_MODEL = \"./output\"\n",
        "CUDA_LEARN = False\n",
        "TRAIN_DATASET = \"./train_dataset\"\n",
        "TEST_DATASET = \"./test_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "j-Byam1XZSHL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: telegram-send==0.34 in /usr/local/lib/python3.10/dist-packages (0.34)\n",
            "Requirement already satisfied: python-telegram-bot>=13.0 in /usr/local/lib/python3.10/dist-packages (from telegram-send==0.34) (13.5)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from telegram-send==0.34) (1.4.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from telegram-send==0.34) (0.4.6)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot>=13.0->telegram-send==0.34) (3.6.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot>=13.0->telegram-send==0.34) (6.2)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot>=13.0->telegram-send==0.34) (2023.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot>=13.0->telegram-send==0.34) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=13.0->telegram-send==0.34) (1.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=13.0->telegram-send==0.34) (4.3)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=13.0->telegram-send==0.34) (67.6.1)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot>=13.0->telegram-send==0.34) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot>=13.0->telegram-send==0.34) (2023.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: python-telegram-bot==13.5 in /usr/local/lib/python3.10/dist-packages (13.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.5) (2022.12.7)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.5) (3.6.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.5) (6.2)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot==13.5) (2023.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.5) (1.16.0)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.5) (67.6.1)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from APScheduler==3.6.3->python-telegram-bot==13.5) (4.3)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot==13.5) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal>=1.2->APScheduler==3.6.3->python-telegram-bot==13.5) (2023.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install telegram-send==0.34\n",
        "!pip3 install python-telegram-bot==13.5\n",
        "!pip install python-dotenv==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "\n",
        "path='../config/.env'\n",
        "load_dotenv(dotenv_path=path, verbose=True)\n",
        "\n",
        "PROJECT_DIR = os.environ.get(\"PROJECT_ROOT\", \"\")\n",
        "TG_NOTIFY_BOT_TOKEN = os.environ.get(\"TG_NOTIFY_BOT_TOKEN\", \"\")\n",
        "TRAIN_STYLE_PATH = os.environ.get(\"TRAIN_STYLE_PATH\", \"./styles/mosaic.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Talk with the \u001b[36mBotFather\u001b[0m on Telegram (https://telegram.me/BotFather), create a bot and insert the token\n",
            "\u001b[35m❯ \u001b[0mConnected with \u001b[36mjupyter_s1sharp_bot\u001b[0m.\n",
            "\n",
            "Please add \u001b[36mjupyter_s1sharp_bot\u001b[0m on Telegram (https://telegram.me/jupyter_s1sharp_bot)\n",
            "and send it the password: \u001b[1m32612\u001b[0m\n",
            "\n",
            "\u001b[32mCongratulations s1Sharp! \n",
            "telegram-send is now ready for use!\u001b[0m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!echo \"$TG_NOTIFY_BOT_TOKEN\"> ts.conf\n",
        "!telegram-send --configure < ts.conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!telegram-send \"Operation ok\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "tg_file = \"./ts.conf\"\n",
        "os.environ[\"TG_SEND_FILE\"] = tg_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[0m"
          ]
        }
      ],
      "source": [
        "!telegram-send --file \"$TG_SEND_FILE\"\n",
        "!telegram-send -f {tg_file} --caption \"Hello\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxgZK3t3pIE3",
        "outputId": "f2012116-210f-4cfa-c2a0-10edfacd76cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-04-08 20:59:12--  http://images.cocodataset.org/zips/train2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.225.137, 52.217.230.33, 52.217.122.25, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.225.137|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510573713 (13G) [application/zip]\n",
            "Saving to: ‘train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  59.8MB/s    in 3m 40s  \n",
            "\n",
            "2023-04-08 21:02:52 (58.6 MB/s) - ‘train2014.zip’ saved [13510573713/13510573713]\n",
            "\n",
            "--2023-04-08 21:02:52--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.108.67, 52.217.206.233, 54.231.128.129, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.108.67|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  66.2MB/s    in 97s     \n",
            "\n",
            "2023-04-08 21:04:29 (65.3 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip # https://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip # https://msvocds.blob.core.windows.net/coco2014/val2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Uk8_sq3swOR"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {TRAIN_DATASET} {TEST_DATASET}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us67KTo0pPG4",
        "outputId": "0fb78664-7a29-42e8-cb5c-3cb6cdf2df5b"
      },
      "outputs": [],
      "source": [
        "!unzip -q train2014.zip -d {TRAIN_DATASET}\n",
        "!unzip -q val2014.zip -d {TEST_DATASET}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOqsN3nvE9te",
        "outputId": "0e447a2f-d9d5-4230-c12a-9043ba5cf041"
      },
      "outputs": [],
      "source": [
        "!pip3 install numpy opencv_python Pillow torch torchfile torchvision tqdm av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmyUIhN6pVMM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "# import utils\n",
        "# from net import Net, Vgg16\n",
        "\n",
        "# from option import Options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plboUlHscetk"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Mean and standard deviation used for pre-trained PyTorch models\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "def gram_matrix(y):\n",
        "    \"\"\" Returns the gram matrix of y (used to compute style loss) \"\"\"\n",
        "    (b, c, h, w) = y.size()\n",
        "    features = y.view(b, c, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t) / (c * h * w)\n",
        "    return gram\n",
        "\n",
        "\n",
        "def train_transform(image_size):\n",
        "    \"\"\" Transforms for training images \"\"\"\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(int(image_size * 1.15)),\n",
        "            transforms.RandomCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ]\n",
        "    )\n",
        "    return transform\n",
        "\n",
        "\n",
        "def style_transform(image_size=None):\n",
        "    \"\"\" Transforms for style image \"\"\"\n",
        "    resize = [transforms.Resize(size=(image_size, image_size))] if image_size else []\n",
        "    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "    return transform\n",
        "\n",
        "\n",
        "def denormalize(tensors):\n",
        "    \"\"\" Denormalizes image tensors using mean and std \"\"\"\n",
        "    for c in range(3):\n",
        "        tensors[:, c].mul_(std[c]).add_(mean[c])\n",
        "    return tensors\n",
        "\n",
        "\n",
        "def deprocess(image_tensor):\n",
        "    \"\"\" Denormalizes and rescales image tensor \"\"\"\n",
        "    image_tensor = denormalize(image_tensor)[0]\n",
        "    image_tensor *= 255\n",
        "    image_np = torch.clamp(image_tensor, 0, 255).cpu().numpy().astype(np.uint8)\n",
        "    image_np = image_np.transpose(1, 2, 0)\n",
        "    return image_np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WDPTN_tJ9ZC"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "class Options():\n",
        "    def __init__(self):\n",
        "        self.parser = argparse.ArgumentParser(description=\"Parser for Fast-Neural-Style\")\n",
        "        self.parser.add_argument(\"--dataset_path\", type=str, required=True, help=\"path to training dataset\")\n",
        "        self.parser.add_argument(\"--style_image\", type=str, default=\"styles/mosaic.jpg\", help=\"path to style image with .jpeg format\")\n",
        "        self.parser.add_argument(\"--epochs\", type=int, default=1, help=\"number of training epochs\")\n",
        "        self.parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size for training\")\n",
        "        self.parser.add_argument(\"--image_size\", type=int, default=256, help=\"size of training images\")\n",
        "        self.parser.add_argument(\"--style_size\", type=int, help=\"size of style image\")\n",
        "        self.parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
        "        self.parser.add_argument(\"--lambda_content\", type=float, default=1e5, help=\"Weight for content loss\")\n",
        "        self.parser.add_argument(\"--lambda_style\", type=float, default=1e10, help=\"Weight for style loss\")\n",
        "        self.parser.add_argument(\"--checkpoint_model\", type=str, help=\"Optional path to checkpoint model\")\n",
        "        self.parser.add_argument(\"--checkpoint_interval\", type=int, default=2000, help=\"Batches between saving model\")\n",
        "        self.parser.add_argument(\"--sample_interval\", type=int, default=1000, help=\"Batches between saving image samples\")\n",
        "\n",
        "    def parse(self):\n",
        "        return self.parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5s90uh0KBZH"
      },
      "outputs": [],
      "source": [
        "import sys,os,argparse\n",
        "from IPython.display import HTML\n",
        "CONFIG_FILE = '.config_ipynb'\n",
        "if os.path.isfile(CONFIG_FILE):\n",
        "    with open(CONFIG_FILE) as f:\n",
        "        sys.argv = f.read().split()\n",
        "else:\n",
        "    sys.argv = ['train.py',\n",
        "                '--dataset_path', TRAIN_DATASET,\n",
        "                '--style_image', TRAIN_STYLE_PATH\n",
        "                ]\n",
        "\n",
        "args = Options().parse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P5J1Y8ze5cj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import namedtuple\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class VGG16(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(VGG16, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            ConvBlock(3, 32, kernel_size=9, stride=1),\n",
        "            ConvBlock(32, 64, kernel_size=3, stride=2),\n",
        "            ConvBlock(64, 128, kernel_size=3, stride=2),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ResidualBlock(128),\n",
        "            ConvBlock(128, 64, kernel_size=3, upsample=True),\n",
        "            ConvBlock(64, 32, kernel_size=3, upsample=True),\n",
        "            ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True),\n",
        "            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x) + x\n",
        "\n",
        "\n",
        "class ConvBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "        )\n",
        "        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None\n",
        "        self.relu = relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            x = F.interpolate(x, scale_factor=2)\n",
        "        x = self.block(x)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        if self.relu:\n",
        "            x = F.relu(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "from subprocess import Popen\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "def resolve_chat_id():\n",
        "    url = f'https://api.telegram.org/bot{TG_NOTIFY_BOT_TOKEN}/getUpdates'\n",
        "    response = requests.get(url)\n",
        "    res = response.json().get('result')\n",
        "    if res is not None and len(res) > 0:\n",
        "        try:\n",
        "            res = res[0]['message']['from']['id']\n",
        "        except:\n",
        "            pass\n",
        "    return res\n",
        "\n",
        "def send_document(filename, token, chat_id, text):\n",
        "    url = f'https://api.telegram.org/bot{token}/sendDocument'\n",
        "    data = {'chat_id': chat_id, 'caption': text}\n",
        "    with open(filename, 'rb') as f:\n",
        "        files = {'document': f}\n",
        "        response = requests.post(url, data=data, files=files)\n",
        "        print(response.json())\n",
        "\n",
        "\n",
        "def send_file_to_tg_by_subprocess(filename:str) -> None:\n",
        "    import subprocess\n",
        "    try:\n",
        "        path = Path(filename)\n",
        "        full_path = path.resolve(strict=True)\n",
        "        subprocess.run(['telegram-send', '--file', full_path])\n",
        "    except FileNotFoundError:\n",
        "        print(\"file not exists\")\n",
        "        pass\n",
        "        # doesn't exist\n",
        "\n",
        "\n",
        "def send_file_to_tg(filename:str, text:str = None) -> None:\n",
        "    try:\n",
        "        path = Path(filename)\n",
        "        full_path = path.resolve(strict=True)\n",
        "        print(full_path)\n",
        "        if text is None:\n",
        "            p = Popen(['telegram-send', '-i', full_path]) # something long running\n",
        "        else:\n",
        "            p = Popen(['telegram-send', '-i', full_path, '--caption', text]) # something long running\n",
        "    except FileNotFoundError:\n",
        "        print(\"file not exists\")\n",
        "        pass\n",
        "        # doesn't exist\n",
        "\n",
        "\n",
        "def send_file_to_tg(filename:str, text:str = None) -> None:\n",
        "    try:\n",
        "        path = Path(filename)\n",
        "        full_path = path.resolve(strict=True)\n",
        "        print(full_path)\n",
        "        if text is None:\n",
        "            p = Popen(['telegram-send', '-f', full_path]) # something long running\n",
        "        else:\n",
        "            p = Popen(['telegram-send', '-f', full_path, '--caption', text]) # something long running\n",
        "    except FileNotFoundError:\n",
        "        print(\"file not exists\")\n",
        "        pass\n",
        "        # doesn't exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "573811408"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resolve_chat_id()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/src/notebooks/ts.conf\n"
          ]
        }
      ],
      "source": [
        "send_file_to_tg(\"./ts.conf\", 'ho')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPlrNA0bcPgM",
        "outputId": "d40899df-5d2f-4b4f-e5c2-65b3e4f8fbec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/1] [Batch 10125/40504] [Content: 1150463.12 (1332310.14) Style: 319875.41 (926799.25) Total: 1470338.50 (2259109.39)]"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    style_name = args.style_image.split(\"/\")[-1].split(\".\")[0]\n",
        "    os.makedirs(f\"images/outputs/{style_name}-training\", exist_ok=True)\n",
        "    os.makedirs(f\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create dataloader for the training data\n",
        "    train_dataset = datasets.ImageFolder(args.dataset_path, train_transform(args.image_size))\n",
        "    dataloader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "\n",
        "    # Defines networks\n",
        "    transformer = TransformerNet().to(device)\n",
        "    vgg = VGG16(requires_grad=False).to(device)\n",
        "\n",
        "    # Load checkpoint model if specified\n",
        "    if args.checkpoint_model:\n",
        "        transformer.load_state_dict(torch.load(args.checkpoint_model))\n",
        "\n",
        "    # Define optimizer and loss\n",
        "    optimizer = Adam(transformer.parameters(), args.lr)\n",
        "    l2_loss = torch.nn.MSELoss().to(device)\n",
        "\n",
        "    # Load style image\n",
        "    style = style_transform(args.style_size)(Image.open(args.style_image))\n",
        "    style = style.repeat(args.batch_size, 1, 1, 1).to(device)\n",
        "\n",
        "    # Extract style features\n",
        "    features_style = vgg(style)\n",
        "    gram_style = [gram_matrix(y) for y in features_style]\n",
        "\n",
        "    # Sample 8 images for visual evaluation of the model\n",
        "    image_samples = []\n",
        "    for path in random.sample(glob.glob(f\"{args.dataset_path}/*/*.jpg\"), 8):\n",
        "        image_samples += [style_transform(args.image_size)(Image.open(path))]\n",
        "    image_samples = torch.stack(image_samples)\n",
        "\n",
        "    def save_sample(batches_done):\n",
        "        \"\"\" Evaluates the model and saves image samples \"\"\"\n",
        "        transformer.eval()\n",
        "        with torch.no_grad():\n",
        "            output = transformer(image_samples.to(device))\n",
        "        image_grid = denormalize(torch.cat((image_samples.cpu(), output.cpu()), 2))\n",
        "        save_image(image_grid, f\"images/outputs/{style_name}-training/{batches_done}.jpg\", nrow=4)\n",
        "        transformer.train()\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        epoch_metrics = {\"content\": [], \"style\": [], \"total\": []}\n",
        "        for batch_i, (images, _) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images_original = images.to(device)\n",
        "            images_transformed = transformer(images_original)\n",
        "\n",
        "            # Extract features\n",
        "            features_original = vgg(images_original)\n",
        "            features_transformed = vgg(images_transformed)\n",
        "\n",
        "            # Compute content loss as MSE between features\n",
        "            content_loss = args.lambda_content * l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n",
        "\n",
        "            # Compute style loss as MSE between gram matrices\n",
        "            style_loss = 0\n",
        "            for ft_y, gm_s in zip(features_transformed, gram_style):\n",
        "                gm_y = gram_matrix(ft_y)\n",
        "                style_loss += l2_loss(gm_y, gm_s[: images.size(0), :, :])\n",
        "            style_loss *= args.lambda_style\n",
        "\n",
        "            total_loss = content_loss + style_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_metrics[\"content\"] += [content_loss.item()]\n",
        "            epoch_metrics[\"style\"] += [style_loss.item()]\n",
        "            epoch_metrics[\"total\"] += [total_loss.item()]\n",
        "\n",
        "            sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    args.epochs,\n",
        "                    batch_i * args.batch_size,\n",
        "                    len(train_dataset),\n",
        "                    content_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"content\"]),\n",
        "                    style_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"style\"]),\n",
        "                    total_loss.item(),\n",
        "                    np.mean(epoch_metrics[\"total\"]),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            batches_done = epoch * len(dataloader) + batch_i + 1\n",
        "            if batches_done % args.sample_interval == 0:\n",
        "                save_sample(batches_done)\n",
        "                send_file_to_tg(checkpoint_filename, checkpoint_filename)\n",
        "\n",
        "            if args.checkpoint_interval > 0 and batches_done % args.checkpoint_interval == 0 or :\n",
        "                style_name = os.path.basename(args.style_image).split(\".\")[0]\n",
        "                checkpoint_filename = f\"checkpoints/{style_name}_{batches_done}.pth\"\n",
        "                torch.save(transformer.state_dict(), f\"checkpoints/{style_name}_{batches_done}.pth\")\n",
        "                send_file_to_tg(checkpoint_filename, checkpoint_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDdK49gYjcJ8",
        "outputId": "96e10305-3a21-4121-8b11-d73b8d83a149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   1436   12917  107579\n"
          ]
        }
      ],
      "source": [
        "!ls -al ./dataset/train2014 | wc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cTxdv1kkcJG",
        "outputId": "cca07aca-e822-4c6d-d23f-f6bc4a4b20d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 96\n",
            "drwxr-xr-x 3 root root  4096 Apr  8 01:53 .\n",
            "drwxr-xr-x 1 root root  4096 Apr  8 01:29 ..\n",
            "drwxr-xr-x 2 root root 90112 Apr  8 01:51 train2014\n"
          ]
        }
      ],
      "source": [
        "!ls -al ./dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j7mZCzXkPk7"
      },
      "outputs": [],
      "source": [
        "!rm -rf ./dataset/train2014_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E7IIe7ajzhV"
      },
      "outputs": [],
      "source": [
        "!mv ./dataset/train2014_2/COCO_train2014_00000022* ./dataset/train2014/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs5fzOHa2kEI"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer.state_dict(), f\"checkpoints/{'mosaic'}_{358}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jja4e-t23YP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import argparse\n",
        "import os\n",
        "import tqdm\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "os.makedirs(\"images/outputs\", exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = style_transform()\n",
        "\n",
        "# Define model and load model checkpoint\n",
        "transformer = TransformerNet().to(device)\n",
        "transformer.load_state_dict(torch.load('/content/checkpoints/mosaic_4000.pth'))\n",
        "transformer.eval()\n",
        "\n",
        "img_path = '/content/styles/picasso_selfport1907.jpg'\n",
        "\n",
        "# Prepare input\n",
        "image_tensor = Variable(transform(Image.open(img_path))).to(device)\n",
        "image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "# Stylize image\n",
        "with torch.no_grad():\n",
        "    stylized_image = denormalize(transformer(image_tensor)).cpu()\n",
        "\n",
        "# Save image\n",
        "fn = img_path.split(\"/\")[-1]\n",
        "save_image(stylized_image, f\"images/outputs/stylized-{fn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k0VJYewF3cXE",
        "outputId": "b10eab1f-68d6-4625-d109-ccd655ecbac7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'images/outputs/stylized-the_scream.jpg'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f\"images/outputs/stylized-{fn}\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
